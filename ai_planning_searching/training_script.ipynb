{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d252299f-432c-4333-ae15-7f1eaf7715af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import transformers\n",
    "import pdb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e215c7-9249-46f9-8ae5-ee3449b385b3",
   "metadata": {},
   "source": [
    "# **Finetuning an Encoder-Decoder model (T5) on an autoregressive language modeling task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58dc171-5e8a-461d-8961-38ad84d9012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_init_model_optimizer_tokenizer():\n",
    "    pretrained_weights = 'google-t5/t5-small'\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_weights)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n",
    "    # models are usually loaded in eval() mode, so set this to train()\n",
    "    model.train()\n",
    "    # initialize the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    # using weight decay\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "    return model, optimizer, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c12df0-fd8e-4bd9-9299-73bcb9eaef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annhe/anaconda3/envs/agency/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/annhe/anaconda3/envs/agency/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, tokenizer = load_init_model_optimizer_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866a3d8-c4d4-4f5b-854a-b19c53b88203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5192f904-e1e7-4470-b335-c8fad30d40a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This dataset is designed to generate lyrics with HuggingArtists.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"huggingartists/taylor-swift\", split=\"train\")\n",
    "train_dataset.info.description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cce262-80d6-4bb8-8ba3-c8f30d09bc4a",
   "metadata": {},
   "source": [
    "Let's display an entry of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f647ea-aa63-4226-99fe-2406607b0c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Car rides to Malibu\\nStrawberry ice cream, one spoon for two\\nAnd tradin jackets\\nLaughin’ bout how small it looks on you\\nWatching reruns of Glee\\nBein annoying, singin’ in harmony\\nI bet shes braggin to all her friends, sayin youre so unique, hmm\\nSo when you gonna tell her that we did that, too?\\nShe thinks its special, but its all reused\\nThat was our place, I found it first\\nI made the jokes you tell to her when shes with you\\nDo you get déjà vu when she’s with you?\\nDo you get déjà vu? Hmm\\nDo you get déjà vu, huh?\\nDo you call her, almost say my name?\\n’Cause lets be honest, we kinda do sound the same\\nAnother actress\\nI hate to think that I was just your type\\nAnd I bet that she knows Billy Joel\\n’Cause you played her Uptown Girl\\nYoure singin it together\\nNow I bet you even tell her how you love her\\nIn between the chorus and the verse \\nSo when you gonna tell her that we did that, too?\\nShe thinks its special, but it’s all reused\\nThat was the show we talked about\\nPlayed you the songs shes singing now when shes with you\\nDo you get déjà vu when shes with you?\\nDo you get déjà vu? \\nDo you get déjà vu?\\nStrawberry ice cream in Malibu\\nDont act like we didnt do that shit, too\\nYoure tradin jackets like we used to do\\nPlay her piano, but she doesnt know \\nThat I was the one who taught you Billy Joel \\nA different girl now, but theres nothing new\\nI know you get déjà vu\\nI know you get déjà vu\\nI know you get déjà vu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "df = pd.DataFrame(train_dataset[:1])\n",
    "\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "077c3a85-2fb7-4aea-a90e-14b0edcf98a8",
   "metadata": {},
   "source": [
    "Make use of the .map() function to transform each training batch into a batch of model inputs.\n",
    "\n",
    "The \"text\" field needs to be tokenized to \"input_ids\"\n",
    "And the \"text\" field needs to be rightshifted and padded to \"decoder_input_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56da4580-f253-4013-9b85-bf359ae1c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length = 512\n",
    "decoder_max_length = 512\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    text = batch['text']\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    output_text = [b[1:] + tokenizer.eos_token for b in text]\n",
    "    outputs = tokenizer(output_text, padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch['input_ids'] = inputs.input_ids\n",
    "    batch['attention_mask'] = inputs.attention_mask\n",
    "    batch['decoder_input_ids'] = outputs.input_ids\n",
    "    batch['decoder_attention_mask'] = outputs.attention_mask\n",
    "    batch['labels'] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    # batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567483da-74a3-42e8-8138-9d429f8faef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset():\n",
    "    train_dataset = load_dataset(\"huggingartists/taylor-swift\", split=\"train\")\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12af0e5d-5742-4e80-8134-b3b68036b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset()\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0311196a-0c4c-467b-9682-79efe92aadc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
       "    num_rows: 762\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645f011-9abb-4af2-9874-97c9270d4d91",
   "metadata": {},
   "source": [
    "Let's convert the data to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a83a6190-9b5d-438c-9539-716e10b14623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca33e734-ecba-4538-8c55-018dd9096baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in model.generate you need to set return_dict_in_generate=False\n",
    "to return torch.LongTensor\n",
    "\"\"\"\n",
    "def sampling_loop(model, input_ids, attention_mask, num_decode_steps=10):\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=num_decode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "297e8f06-b505-43f4-a2a8-8fb367a70295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DOING\n",
    "\"\"\"\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b5387c2-89b3-4374-be45-9300ca064443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=train_dataset,           # evaluation dataset\n",
    "    tokenizer=tokenizer,                 # tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b538c0e-7f8a-4f20-a1a0-e329334c8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-05 10:18:57,225] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to mps (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2024-09-05 10:18:57,889] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fdc6c3-4ee9-43e6-90e8-2fd91bce5f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
